# -*- coding: utf-8 -*-
"""BERT_trainer.py .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CZAC5QzLaIQl7Gas2if7jZr_XqwkZXpz
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict
import torch

# load dataset
df = pd.read_csv('medical_qa_dataset.csv')

# Create a list of diseases
diseases = ["diabetes", "cancer", "heart disease", "asthma", "stroke", "hypertension", "pneumonia"]

# multi-label classification
def classify_disease(question):
    labels = [disease for disease in diseases if disease.lower() in question.lower()]
    return labels

df['disease_labels'] = df['question'].apply(classify_disease)

# convert disease labels to binary format
def label_to_binary(disease_labels):
    return [1 if disease in disease_labels else 0 for disease in diseases]

df['binary_labels'] = df['disease_labels'].apply(label_to_binary)

# tokenize dataset
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples['question'], padding="max_length", truncation=True, max_length=256)

# convert dataFrame to Hugging Face dataset
dataset = Dataset.from_pandas(df)
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Split the dataset into training and validation
train_dataset, eval_dataset = train_test_split(tokenized_dataset, test_size=0.2)

# Convert to Hugging Face DatasetDict
dataset_dict = DatasetDict({
    'train': Dataset.from_pandas(train_dataset),
    'eval': Dataset.from_pandas(eval_dataset)
})

# load the BERT model for multi-label classification
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(diseases), problem_type="multi>

# training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    logging_dir="./logs",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    save_steps=1000,
    save_total_limit=2,
    load_best_model_at_end=True
)

# Set up the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset_dict['train'],
    eval_dataset=dataset_dict['eval'],
    tokenizer=tokenizer
)

# Fine-tune the model
  trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()

# Calculate precision, recall, and F1 score
predictions = eval_results.get("eval_preds", [])
labels = eval_results.get("eval_labels", [])

# Convert predictions to binary values
predictions = np.argmax(predictions, axis=-1)
precision = precision_score(labels, predictions, average='weighted')
recall = recall_score(labels, predictions, average='weighted')
f1 = f1_score(labels, predictions, average='weighted')

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")